\newpage
\chapter{Methodology}
\label{sec:proposal}
\noindent
This master thesis proposes an application of deep CNNs to a task of counting objects in the image. The provided system is to address all the aforestated issues which object detection and counting applications have encountered. Such as:
\begin{itemize}
	\item Being prone to error in noisy or crowded scenes with a noticeable occlusion. 
	\item Establishing the viability of a privacy-preserving approach. 
	\item Painstaking hand-crafted image features which are highly dependent on the object class. 
	\item Scrupulous data annotation for manifold data. 
\end{itemize} 
In addition, the deficit in the state-of-the-art \cite{segui2015learning} which would be the applicability and performance of a system trained with synthetic dataset in real world counting problem\cite{chan2008privacy}. The novelty of our approach compare to the state of the art is that we hypothesize that features learned by training a counting deep CNN on a synthetic dataset, are representative enough to count the number of object of interest in a real dataset. We tackle this task as a regression problem. 
To the best of our knowledge, the proposed work would be the first one in which a counting system trained with synthetic images is able to be incorporated in real-world similar counting problems.

Henceforth, in the rest of this chapter, we justify our methodology along with a comparison to state-of-the-art from different aspects such as method selection, architecture, dataset and its application.   


%This master thesis proposes an application of deep convolutional neural networks to a task of counting objects in the image. The novelty of our approach compare to the state of the art is that we hypothesize that features learned by training a counting deep CNN on a synthetic dataset, are representative enough to count the number of object of interest in a real dataset. This approach has been taken to immensely reduce feature detection and data annotation efforts in the state of the art. Furthermore, due to the synthetic nature state of the art\cite{segui2015learning}, we would to examine the performance of such systems on a larger scale real world counting problem. 

%Hence, to verify our hypothesis and resolve the deficits of the state-of-the-art, first we scale up  \citealt*{segui2015learning}' work along with a different architecture to observe if we attain promising results, and then we compare the performance of our model with the results \citeauthor{chan2008privacy} achieved on real dataset\cite{chan2008privacy}.  
%To the best of our knowledge, the proposed work would be the first one in which a counting system trained with synthetic images is able to be incorporated in real-world similar counting problems.

%Henceforth, in the course of this chapter, we explain different components of our system more in details

\section{Method selection}

For a long time in Computer Vision, there has been a prevailing paradigm in which we have a set of feature descriptors such SIFT \cite{lowe1999object}, HOG\cite{dalal2005histograms}, SURF\cite{bay2006surf} and many more to that can be extracted from the image with possible higher level feature building following by a classifier like Support Vector Machines (SVM) \cite{vapnik1964note, boser1992training}. In fact, for the most part, these features are not learned, but hand-crafted by some vision experts. However, they do have indeed descent performance. For instance, in one of the most successful works in object detection, in \cite{felzenszwalb2010object} the author essentially introduces a linear classifier on top of HOG features, or regarding classification approaches that work quite well, \citeauthor{yu2010object} use all manners of features (HOG, SIFT, Color SIFT, etc) extracted from the images and consequently obtain impressive results. 

\indent The analysis of these works develop this question that in order to improve the vision systems accuracy, in which part of the system should we focus on? Should we try to enhance classifiers, should we increase the amount of data or we had better provide finer features? \citeauthor*{parikh2010role} in \cite{parikh2010role} analyze the role of features by taking some of the quite successful past deformable models\cite{albrecht2015deformable}, and replace some components of them with humans. they present identical learning tasks \textit{i.e.} the same feature representation and the same training data, to machines and humans which allows drawing a comparison between the two. The author concludes that features are the main factor contributing to superior human performance. Furthermore, in \cite{gehler2009feature}, compared 39 different learning kernels with different combination features to see which kernel outperforms the rest and how it should be weighted. Although they got a big jump over the existing methods, the analysis of their results shows that the gain they obtained from the learning operators is not as dramatic as the improvement they achieved from the features itself. 

\indent Therefore, since the features are doing most of the works in these algorithms, if we improve those, we can attain better algorithms. The difficulty of feature improvement and high cost of numerous features computation on each image brought us into the application of deep learning in order to learn the features themselves rather than hand-crafting them. 

In deep learning techniques, we essentially have a hierarchy of feature extractors which attempts to model high-level abstractions in data\cite{deng2014deep, bengio2009learning, bengio2013representation, arel2010deep, schmidhuber2015deep} where each layer of hierarchy extracts features from output of previous layer. Designing such trainable feature extractors, we would be able to build a multi-stage model which goes all the way from image pixels up to a high-level feature vector which we can feed it to a standard classifier. Thus, from the family of DL, we select deep CNNs due to its' success in may recent vision problems such as \cite{cirecsan2011convolutional, ciresan2015multi, wan2013regularization, cirecsan2012multi}, their capacity of knowledge transfer for a number of tasks and also the ability for performing different tasks simultaneously, even when it has been trained for only one task\cite{zhou2014learning}. 

\section{Architecture}

CNNs actually have a long heritage. The origin of CNN comes from \cite{hubel1962receptive} where first simple cells detect local features and afterwards, complex cells pool the outputs of simple cells within a retinotopic neighborhood. Also, \citeauthor*{fukushima1975cognitron} in \cite{fukushima1975cognitron, fukushima1980neocognitron} introduced a similar architecture with filtering layers following by pooling layers. However, the the first deep CNN architecture was designed by \citealt{lecun1989backpropagation} who demonstrated that this kind of architecture can perform quite well for vision tasks like digit recognition.

Following the same structure of classical CNNs, in our architecture, the image pixels are fed to a convolutional layer, where relatively small filters (windows) shift over the image (not necessarily pixel by pixel, different stride can be taken) and produce feature maps. Since convolution is a linear operation, in order to make the model non-linear, feature maps are passed to rectified linear units (ReLU) \cite{nair2010rectified} which is applied to each element of the feature map independently. ReLU is currently in favor given the fact that it fastens the learning process by massively simplifying back propagation, and also avoids saturation issues( when the weighted sum is big, the output of the \textit{tanh} or \textit{sigmoid} activation functions saturates and the gradient tends to zero. See \cite{hansen1990neural, amit1987statistical} for more details.). 

\indent Thereafter, a pooling layer takes a special region of the obtained feature maps and take the maximum (or sum over the pixels of the neighborhood) pixel value as the strongest structure of that neighborhood. We chose Max-pooling since it tends to give more discrimination of the features\cite{boureau2010theoretical}. In our model, we use \textit{spatial} pooling. However, depending on the problem we are trying to solve, pooling might be done within feature maps\cite{goodfellow2013maxout}.  The main advantage of pooling layers in the architecture can be that it results in invariance to small transformations. In addition to that, as we go up in CNNs, thanks to pooling, each of the units essentially has a larger receptive fields looking back the input so that at the top high-level layers, each unit looks at the entire scene.  

To improve the model performance, after each pooling layer in the proposed model, we used local response normalization (LRN)layer to apply a contrast normalization. This type of normalization was introduced and used for the first time in \cite{krizhevsky2012imagenet}. Empirically, using LRN in the architecture seems to help improving the results \cite{jarrett2009best, krizhevsky2012imagenet}. Basically LRN introduces a local competition between features, in a way that it picks a single location in the output and it looks at a special neighborhood around that location in the input. Then it computes the local mean in that region, weight it by the Gaussian distribution, translate and make it to corresponding vectors with local mean = 0 and local standard deviation = 1(the chosen values are problem dependent. In our case, we chose $\mu = 0$ \& $std = 1 $ ). This brings out more details in the darker regions of the feature maps. 

\indent Moreover, LRN helps to scale activations at each layer better for learning by making energy surface more isotropic. That means that if we use a single learning rate for all the layers, then each gradient step tends to make much more progress\cite{jarrett2009best}. 

And finally on top of the model, we put fully connected layers in order to do either a classification or regression strategy. In our model, since we are casting the problem as a regression problem, a \textit{Euclidean Loss} layer is stated on top of fully connected layers to project the output as the difference between model prediction and the ground truth. 

The above explanation is just to reason the selected architecture and how it can help us to overcome the deficiencies of previous related works. However, the most important fact regarding CNNs' capability to learn features is the deepness and settings of the architecture which will be attentively addressed in the next chapter.  

\section{Datasets} 

Here, starting with a short review of data for CNN in vision, we express a succinct reasoning about the datasets we used in our experiment and later on, in the implementation section, the specifications and preprocessing phases regarding each dataset will be described in details. 

In spite of remarkable  performance of CNN in some simple recognition benchmarks \cite{cirecsan2011convolutional, ciresan2015multi, wan2013regularization, cirecsan2012multi}, until recently the models' performances were poor at more complex datasets \cite{griffin2007caltech} due to lack of labeled input samples to train the network parameters with. It was with the creation of ImageNet\cite{deng2009imagenet} and GPU implementation\cite{krizhevsky2012imagenet} (50x speedup over CPU) that efficiency of deep CNN in vision tasks became crystal clear.

In our work, we use three different datasets for two distinct but related tasks. First we create





%\begin{equation} \label{eq:curvilinear_energy}
%\begin{split}
%I \colon & \mathbb{R}^{D} \phantomarrow{AAAA} {}\mathbb{S}^1\times\mathbb{R}^2\\
%& \phantomarrow{AAA}{p} I(p) =(h,s,i)
%\end{split}
%end{equation}

%\begin{figure}[h!]
	%\centering
	%\begin{minipage}{0.45\textwidth}
		%\centering
		%{\includegraphics[width=0.8\textwidth]{images/image_in_question_modified.png}} % first figure itself
		%\caption{First step of the iteration in the segmentation. The initial cage in blue, the contour in white and the next cage in green with ten times the learning rate of step 1.} \label{fig:image_in_question}
	%\end{minipage}\hfill
	%\begin{minipage}{0.5\textwidth}
		%\centering
		%{\includegraphics[width=1.\textwidth]{images/gradient_plot_edit.png}}
		%\caption{Contribution of the points in a slice of image~\ref{fig:image_in_question} on the direction of the gradient with respect to $v_i$ of the x axis.}\label{fig:plot_in_question}
	%\end{minipage}
	%\centering
