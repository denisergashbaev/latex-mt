\newpage
\chapter{Methodology}\todo{i think this section should be called `Proposal'}
\label{sec:proposal}
\noindent
This master thesis proposes an application of deep CNNs to the task of counting objects in the image. The developed systems will address some of the issues common to other object detection algorithms. These issues include:
\begin{itemize}
	\item Being prone to error in noisy or crowded scenes with a noticeable occlusion. 
	\item Establishing the viability of a privacy-preserving approach. 
	\item Painstaking hand-crafted image features which are highly dependent on the object class. 
	\item Scrupulous data annotation for manifold data. 
\end{itemize} 
In addition, the state-of-the-art \cite{segui2015learning} technology lacks applicability for real-world counting problems due to being tested only on synthetic dataset.

The novelty of our approach compared to the state-of-the-art is that we hypothesize that features learned by training a  deep CNN with synthetic dataset, are representative enough to count the number of object of interest in a real dataset. We tackle this task as a regression problem. 
To the best of our knowledge, the proposed work would be the first one in which a counting system trained with synthetic images is able to be incorporated in real-world similar counting problems.

Henceforth, in the rest of this chapter, we justify our methodology along with a comparison to state-of-the-art from different aspects including method selection, architecture, dataset and its application.   


\section{Method selection}

For a long time in Computer Vision, there has been a prevailing paradigm in which we have a set of feature descriptors such as SIFT \cite{lowe1999object}, HOG \cite{dalal2005histograms}, SURF \cite{bay2006surf}, and others that can be extracted from the image with possible higher level feature building (High-level feature detection algorithms are more in tune with the way we detect objects in real life), following by a classifier like Support Vector Machines (SVM) \cite{vapnik1964note, boser1992training}. In fact, for the most part, these features are not learned, but hand-crafted by some vision experts. However, they do indeed demonstrate a decent performance. For instance, in one of the most successful works in object detection \cite{felzenszwalb2010object}, the author essentially introduces a linear classifier on top of HOG features, or regarding classification approaches that work quite well, \citeauthor{yu2010object} use all manners of features (HOG, SIFT, Color SIFT, etc) extracted from the images and consequently obtain impressive results. 

\indent The analysis of these works leads to the question of what we should focus on in order to improve the vision systems accuracy. Should we try to enhance classifiers, should we increase the amount of data or we should rather provide finer features? \citeauthor*{parikh2010role} in \cite{parikh2010role} analyze the role of features by taking some of the quite successful past deformable models \cite{albrecht2015deformable} (deformable models are curves or surfaces defined within an image domain whose deformations are determined by the displacement of a discrete number of control points along the curve \cite{xu2000image}), and replace some components of them with humans. They present identical learning tasks \textit{i.e.} the same feature representation and the same training data to machines and humans which allows drawing a comparison between the two. The author concludes that features are the main factor contributing to superior human performance.

Furthermore, in \cite{gehler2009feature}, the authors compared 39 different learning kernels with various combination of features to see which kernel outperforms the rest and how it should be weighted. Although they attained a big improvement over the existing methods, the analysis of their results shows that the gain they obtained from the learning operators is not as dramatic as the improvement they achieved from the features itself. 

\indent Therefore, since the features are providing the biggest impact in these algorithms, if we improve those features, we can acquire better algorithms. The difficulty of feature improvement and high cost of numerous feature computations on each image brought us into the application of deep learning in order to learn the features themselves rather than hand-crafting them. 

In deep learning techniques, we essentially have a hierarchy of feature extractors which attempts to model high-level abstractions in data \cite{deng2014deep, bengio2009learning, bengio2013representation, arel2010deep, schmidhuber2015deep}, where each layer of hierarchy extracts features from output of previous layer. Designing such trainable feature extractors, we would be able to build a multi-stage model which goes all the way from image pixels up to a high-level feature vector which we can be fed to a standard classifier. 

Thus, from the family of DL, we select deep CNNs due to their capacity of knowledge transfer for a number of tasks and also the ability for performing different tasks simultaneously, even when it has been trained for only one task \cite{zhou2014learning}. Moreover, compare to other methods, CNN benefit from the powerful speed-ups on GPU which is fit for implementation of CNN. 

Furthermore, CNN was used to achieve state-of-the-art results in CIFAR-10 object recognition task \cite{cirecsan2012multi}, in \cite{russakovsky2015imagenet}, the training process was done on GPU, and a great success in many other recent vision problems \cite{cirecsan2011convolutional, ciresan2015multi, wan2013regularization}. All these features and advantages of CNN assured us to choose convolutional neural networks as our proposed method

\section{Architecture}

With their long heritage, the origins of CNN trace back to \cite{hubel1962receptive} where first simple cells (brain cell) detect local features and afterwards, complex cells pool the outputs of simple cells within a retinotopic\footnote{Retinotopy is the mapping of visual input from the retina to neurons, particularly those neurons within the visual stream. For clarity, 'retinotopy' can be replaced with 'retinal mapping', and 'retinotopic' with 'retinally mapped}neighborhood (see \cite{hubel1962receptive} for more detailed explanation). Also, \citeauthor*{fukushima1975cognitron} in \cite{fukushima1975cognitron, fukushima1980neocognitron} introduced a similar architecture with filtering layers followed by pooling layers. However, the first deep CNN architecture was designed by \citealt{lecun1989backpropagation} who demonstrated that this kind of architecture can perform quite well for vision tasks like digit recognition. A schematic of \citeauthor{lecun1989backpropagation}'s proposed architecture has been depicted in figure~\ref{fig:lecun}

\begin{figure}[H]
	\centering
	{\includegraphics[width=0.7\textwidth]{images/1}}
	\caption{Deep CNN proposed by \citeauthor{lecun1989backpropagation} for MNIST hand-written digits recognition problem.}
	\label{fig:lecun}
\end{figure}

Following the same structure of classical CNNs, in our architecture, the image pixels are fed to a convolutional layer, where relatively small filters (windows) shift over the image (not necessarily pixel by pixel, as a different stride can be taken) and produce feature maps. Since convolution is a linear operation, in order to make the model non-linear, feature maps are passed to rectified linear units (ReLU) \cite{nair2010rectified} which is applied to each element of the feature map independently. 

As discussed in chapter~\ref{sec:actfun}, ReLU is currently favored in the research community given the fact that it fastens the learning process by massively simplifying back propagation, and also avoids saturation issues (when the weighted sum is big, the output of the $\tanh$ or \textit{sigmoid} activation functions saturates and the gradient tends to zero. See \cite{hansen1990neural, amit1987statistical} for more details). 

\indent Thereafter, a max-pooling layer takes a special region of the obtained feature maps and takes the maximum pixel value as the strongest structure of that neighborhood (described in section~\ref{poolinglayer}). We chose Max-pooling since it tends to give more discrimination of the features \cite{boureau2010theoretical}. In our model we use \textit{spatial} pooling. However, depending on the problem we are trying to solve, pooling might be done within feature maps \cite{goodfellow2013maxout}. The main advantage of pooling layers in the architecture can be that it results in invariance to small transformations. In addition to that, as we go up in CNNs, thanks to pooling, each of the units essentially has a larger receptive fields looking back the input so that at the top high-level layers, each unit looks at the entire scene.  

To improve the model performance, after each pooling layer in the proposed model, we used local response normalization layer (LRN) to apply a contrast normalization(explained in details in chapter ~\ref{lrnlayer}). This type of normalization was introduced and used for the first time in \cite{krizhevsky2012imagenet}. Empirically, using LRN in the architecture seems to help improving the results \cite{jarrett2009best, krizhevsky2012imagenet}. 

%%%In essence (explained in details in chapter ~\ref{lrnlayer}), LRN introduces a local competition between features, in a way that it picks a single location in the output and it looks at a special neighborhood around that location in the input. Then it computes the local mean in that region, weighs it by the Gaussian distribution, translates and makes it to corresponding vectors with local mean = 0 and local standard deviation = 1 (the chosen values are problem dependent; in our case, we chose $\mu = 0$ \& $std = 1 $ ). This brings out more details in the darker regions of the feature maps.
%Moreover, LRN helps to scale activations at each layer better for learning by making energy surface more isotropic. That means that if we use a single learning rate for all the layers, then each gradient step tends to make much more progress  \cite{jarrett2009best}. 

And finally on top of the model, we put fully connected layers in order to do either a classification or regression strategy. In our model, since we are casting the problem as a regression problem, a \textit{Euclidean Loss} layer is stated on top of fully connected layers to project the output as the difference between model prediction and the ground truth. Figure ~\ref{fig:proposenet} illustrates our proposed design of deep convolutional neural network with just one convolutional layer in order to portray the basic structure we are going use in this work. 

\begin{figure}[H]
	\centering
	{\includegraphics[width=0.7\textwidth]{images/1}}
	\caption{The proposed design for a CNN with just one convolution layer.}
	\label{fig:proposenet}
\end{figure}

The above explanation describes the reasons behind the selected architecture and how it can help us to overcome the deficiencies of previous related works. However, the most important fact regarding CNNs' capability to learn features is the deepness and settings of the architecture which will be attentively addressed later on (chapter~\ref{imparch}).  

\section{Datasets} 

In this section, starting with a short review of data for CNN in computer vision, we express a succinct reasoning about the datasets we used in our experiments. Later on, in the implementation section, the specifications and preprocessing phases regarding each dataset will be described in detail. 

In spite of remarkable  performance of CNN in some simple recognition benchmarks \cite{cirecsan2011convolutional, ciresan2015multi, wan2013regularization, cirecsan2012multi}, until recently the models' performances were poor at more complex datasets  \cite{griffin2007caltech} due to lack of labeled input samples to train the network parameters with. It was with the creation of ImageNet \cite{deng2009imagenet} and GPU implementation \cite{krizhevsky2012imagenet} ($50\times$ speedup over CPU) that efficiency of deep CNN in vision tasks became crystal clear.

\subsection{MNIST pool of digits dataset}

In this study, we introduce a synthetic dataset using MNIST dataset. The MNIST database of handwritten digits, has a training set of 60,000 examples, and a test set of 10,000 examples. The digits have been size-normalized and centered in a fixed-size image \cite{lecun1999mnist}. In the MNIST dataset, each image contains one hand-written digit of dimension $28\times28$. Figure~\ref{fig:mnistimages} shows a selection of MNIST dataset images.

\begin{figure}[H]
	\centering
	{\includegraphics[width=0.7\textwidth]{images/1}}
	\caption{A selection of MNIST images of hand-written digits.}
	\label{fig:mnistimages}
\end{figure}


\noindent For the first empirical experience in our work, we created a large set of images each containing up to 15 MNIST digits. Images are automatically labeled with the number of even digits in each image. Using this dataset, we can examine the digits' representations learned by our deep CNN are not dependent on the specific task we are dealing with. It is done by using our model's learned features for other tasks and analyze the results. 

\subsection{Synthetic crowd counting dataset}

As we needed a large dataset to train all the parameters of our model for the second task,counting the number of pedestrians, we synthetically created a large dataset of pedestrians in a walkway with maximum 29 people in each image. Achieving success on crowd counting system using CNN on our synthetic dataset, we save enormous amount of time for data annotation and feature detection. 

\subsection{UCSD crowd counting dataset}

However, we need our model to perform well in practice rather than in theoretical experiments. Therefore, we used UCSD crowd counting dataset \cite{chan2008privacy} to validate the performance of our model on it and also make a comparison with their system in \cite{chan2008privacy} to see if we can obtain better results while decreasing feature extraction and labeling efforts. In addition, if we succeed to attain reasonable results on the real UCSD dataset, we have overcome one of the main restrictions of deep learning algorithms usage, which is the necessity for the existence of large amount of data.     


%
%\begin{equation} \label{eq:curvilinear_energy}
%\begin{split}
%I \colon & \mathbb{R}^{D} \phantomarrow{AAAA} {}\mathbb{S}^1\times\mathbb{R}^2\\
%& \phantomarrow{AAA}{p} I(p) =(h,s,i)
%\end{split}
%\end{equation}
%
%\begin{figure}[h!]
%	\centering
%	\begin{minipage}{0.45\textwidth}
%		\centering
%		{\includegraphics[width=0.8\textwidth]{images/1}} % first figure itself
%		\caption{First step of the iteration in the segmentation. The initial cage in blue, the contour in white and the next cage in green with ten times the learning rate of step 1.} \label{fig:image_in_question}
%	\end{minipage}\hfill
%	\begin{minipage}{0.5\textwidth}
%		\centering
%		{\includegraphics[width=1.\textwidth]{images/1}}
%		\caption{Contribution of the points in a slice of image~\ref{fig:image_in_question} on the direction of the gradient with respect to $v_i$ of the x axis.}\label{fig:plot_in_question}
%	\end{minipage}
%	\centering}
%\end{figure}