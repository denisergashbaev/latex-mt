\newpage
\chapter{Experiments and results}
\label{sec:experiments}

Prior to jumping to the final results obtained with our algorithm, this chapter will attempt to give insight into the experiments we designed to verify our hypothesis. We examine our proposed methodology on two different but related scenarios. In the former scenario, we tackle the learning to count even hand-written digits, and for the latter one, crowd counting problem is considered. The findings of the experiments will determine to what extent synthetic data generation could help deep convolutional networks to learn when dealing little amount of data. Hence, this chapter gives full particulars of each experiment, and presents obtained results as compared to similar works.

\section{Learning to count Even-odd hand-written digits}

We intend to explore the features learned when training a deep CNN, in order to understand the underlying representations. To this end, we designed a synthetic problem of counting even digits in images. This experiment clearly illustrates the basic idea behind this work, where we hypothesize that the features learned during this task are:
\begin{enumerate}
\item Efficient descriptors of digits to enable the model to count them.
\item Sufficiently representative to be applied for the digit recognition tasks. 
\end{enumerate}

In this experiment, we feed images of digits into a deep convolutional neural network. Images are labeled with the number of even digits in each image. We took a regression analysis to tackle this problem. Therefore, the output of the network is a single number determining the model's prediction for the amount of even digits present in the images. 
 

\subsection{Dataset} 
As it was meticulously described in section ~\ref{subsubsec:digit}, we synthetically generated Even-odd digits dataset to use for this task. the dataset holds the following properties:
\begin{itemize}
\item Images of MNIST hand-written digits where each image contains up to 15 digits. Also the images are made in gray-scale.
\item Each image has a dimension of $100\times100$ and each digit in the image is $18\times18$ pixels.  
\item A minimum distance of 18 pixels is considered between each two digits (from center to center) in an image to prevent overlapping.
\item The dataset is generated uniformly. It means that there are equal number of images for each amount of even digits in the image. For instance, the number of images with 5 even digits is the same as number of images containing 10 even digits.
\item The dataset of 1,000,000 samples is divided into a training set of 800,000 and a test set of 200,000. 
\end{itemize}

Since we use Caffe platform to perform our experiments, we convert the images into LMDB format. LMDB uses memory-mapped files, so it has the read performance of a pure in-memory database while still offering the persistence of standard disk-based databases, and is only limited to the size of the virtual address space (it is not limited to the size of physical RAM). Therefore, we created two LMDB files for training and testing sets to be fed to the network.  

\subsection{Learning process}

As discussed in chapter~\ref{subsubsec:digitarch}, for learning to count the number of even digits, we designed a deep CNN. Table~\ref{fig:digitnet} shows a review of the network's specification and architecture while table~\ref{hypar3} recaps the settings for the hyper-parameters.


\begin{table}[!htb]
    \begin{minipage}{.5\linewidth}
		\begin{table}[H]
			\centering
			\begin{tabular}{ |p{3.8cm}|p{1.7cm}| }
			\hline 
			\multicolumn{2}{|c|}{\textbf{Network hyper-parameters}} \\
			\hline
			\hline
			\textbf{Hyper-parameters} & \textbf{setting }\\
			\hline
			Learning rate & 0.0001\\
			\hline
			Learning policy    & \textit{Multi\_step} \\
			\hline
			step size   & 40000 \\
			\hline
			Momentum & $\mu = 0.9$\\
			\hline
			Weight decay & 0.0005 \\
			\hline
			Batch size & 256 \\
			\hline
			Iterations & 1600000 \\
			\hline
			\end{tabular}
				\caption{Proposed settings for network's hyper-parameters.}
				\label{hypar3}
		\end{table}
    \end{minipage}%
    \begin{minipage}{.5\linewidth}
		\begin{table}[H]
			\centering
			\begin{tabular}{ |p{2cm}|p{2cm}| }
			\hline 
			\multicolumn{2}{|c|}{\textbf{Network parameters}} \\
			\hline
			\hline
			\textbf{Layers} & \textbf{setting }\\
			\hline
			Conv1 & $20\times15\times15$\\
			\hline
			ReLU1 & max(x,0)  \\
			\hline
			LRN1 & $\alpha$=0.0001, $\beta$=0.75\\
			\hline
			Pool1    & $max(2\times2)$ \\
			\hline
			Conv2 & $50\times3\times3$\\
			\hline
			ReLU2 & max(x,0)  \\
			\hline
			LRN2 & $\alpha$=0.0001, $\beta$=0.75\\
			\hline
			Pool2    & $max(2\times2)$ \\
			\hline
			IP1 & 64 outputs \\
			\hline
			ReLU3 & max(x,0)  \\
			\hline
			IP2 & 1 outputs \\
			\hline
			\end{tabular}
				\caption{Proposed architecture's settings.}
				\label{fig:digitnet}
		\end{table}
		 
    \end{minipage} 
        %\caption{Global caption}
\end{table}

%However, a well-designed network cannot solely guarantee an optimal performance for the model. The responsibilities of learning are divided between the network for yielding loss and gradients, and the optimization methods (solver) and parameters for overseeing the optimization and generating parameter updates. 

%Among Caffe solvers, As described in section~\ref{subsec:sgd}, we use Stochastic Gradient Descent optimization method. Apart from solver method, the solver parameters need to be set attentively in order to optimize the model performance. Table~\ref{tab:digitsolver} summarizes the initial settings for the solver parameters while the upcoming items justify our selections.

%\begin{table}[H]
%	\centering
%	\begin{tabular}{ |p{2cm}|p{2cm}| }
%	\hline 
%	\multicolumn{2}{|c|}{\textbf{Optimization parameters}} \\
%	\hline
%	\hline
%	\textbf{Parameter} & \textbf{value}\\
%	\hline
%	base\_lr & 0.0001\\
%	\hline
%	batch size & 256\\
%	\hline
%	momentum & 0.9  \\
%	\hline
%	weight decay & 0.0005\\
%	\hline
%	step size   & 40000 \\
%	\hline
%	gamma($\gamma$) & 0.1\\
%	\hline
%	iterations & 1,600,000\\
%	\hline
%	\end{tabular}
%		\caption{The solver parameters settings for even digit counting problem.}
%		\label{tab:digitsolver}
%\end{table}


%\begin{itemize}
%\label{solv:param}
%\item \textbf{Learning rate:} After trying different initial values in range of $(10^{-6}, 1)$, we set the basic learning rate to $\alpha = 0.0001$. However, for our experiment we chose \textit{multi-step} learning policy in which, after each \textit{stepsize}=40000 iterations, the learning rate drops by the rate of Gamma $\gamma = 0.1$. This initialization is based on rules of thumb used in \cite{krizhevsky2012imagenet}.
%\item \textbf{Batch size:} Due to the non-complex and low-resolution dataset we are training on, we were able to use batches of size 256 for our training and testing phases.
%\item \textbf{Momentum:} We use momentum $\mu = 0.9$. Because, momentum setting $\mu$ effectively multiplies the size of our updates by a factor of $\frac{1}{1-\mu}$. Hence, changes in momentum and learning rate ought to be accompanied with an inverse correlation. When momentum $\mu = 0.9$, we have an effective update size of 10 since we also drop the learning rate by the factor of $\gamma= 0.1$.
%\item \textbf{Weight decay:} Weight decay as a penalty term to the error function, has a constant value of 0.0005. This decay constant is multiplied to the sum of squared weights.
%\item \textbf{Iterations:} Given the time and hardware we had, we managed to let the system train for 1,600,000 iterations.
%\end{itemize}

\noindent The algorithm trained on a GPU NVIDIA \cite{kirk2007nvidia} Tesla K40 \cite{lindholm2008nvidia}. It took almost 4 days to train 800,000 samples and test over 200,000 images. The learning curves of this process are shown in the figure~\ref{fig:curve}.

\begin{figure}[H]
	\centering
	{\includegraphics[width=0.7\textwidth]{images/1}}
	\caption{}
	\label{fig:curve}
\end{figure}

\textbf{ADD EXPLANATION OF THE LEARNING CURVES}

\noindent Moreover, figure~\ref{fig:conv1tot} illustrates the filters and the output (rectified responses of the shown filters) of the first convolutional layer for one input sample in the network.

\begin{figure*}[h!]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        {\includegraphics[width=0.8\textwidth]{images/conv2feat}}
        \caption{The obtained filters.}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        {\includegraphics[width=0.8\textwidth]{images/conv1out}}
        \caption{The output correspondent to the filters.}
    \end{subfigure}
    \caption{Filters and output of the first convolutional layer in the network.}
    \label{fig:conv1tot}
\end{figure*}

A SHORT EXPLANATION ABOUT THE FIGURES

Accordingly, figure below demonstrates the filters and their corresponding output after the second convolutional layer. 

\begin{figure*}[h!]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        {\includegraphics[width=0.8\textwidth]{images/conv2feat}}
        \caption{The obtained filters.}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        {\includegraphics[width=0.8\textwidth]{images/conv2out}}
        \caption{The output correspondent to the filters.}
    \end{subfigure}
    \caption{Filters and output of the second convolutional layer in the network.}
    \label{fig:conv2tot}
\end{figure*}

 

%Moreover, a schematic of the model with the output of each layer is depicted in figure~\ref{fig:digitnet}.
%%\begin{figure}[H]
%%	\centering
%	{\includegraphics[width=1\textwidth]{images/caffe}}
%	\caption{An MNIST digit classification example of a Caffe network, where blue boxes represent layers and yellow octagons represent data blobs produced by or fed into the layers\cite{jia2014caffe}.}
%	\label{fig:digitnet}
%\end{figure}
\textbf{A SHORT TRANSITIVE PARAGRAPH TO GO TO RESULTS}

\subsection{Experimental results}

In addition to the Euclidean loss function that we implemented in our architecture, we use two other error measurements to evaluate and compare the performance of our model. These measurements are briefly described and justified in below:
\begin{enumerate}
\item \textbf{Mean squared error(MSE)}: Mean squared error is arguably the most important criterion used to evaluate the performance of a predictor or an estimator. The mean squared error is also useful to relay the concepts of bias, precision, and accuracy in statistical estimation. The MSE is the second moment (about the origin) of the error, and thus incorporates both the variance of the estimator and its bias \cite{lehmann1998theory}. The MSE can be estimated by
$$MSE = {\frac{1} {N}{\sum\limits_{i = 1}^N {(\hat{x_{pred}} - x_{obs} } })^{2} } $$
where:\\
\textit{N}: the number of instances\\
\textit{ $\hat{x_{pred}}$}: a vector of N predictions\\
\textit{$x_{obs}$}: a vector of observed value (ground truth) for input x\\

The mathematical benefits of mean squared error are particularly evident in its use for analyzing the performance of linear regression, as it allows one to partition the variation in a dataset into variation explained by the model and variation explained by randomness.
\item \textbf{Mean absolute error(MAE)}: MAE measures the average magnitude of the errors in a set of predictions, without considering their direction. In other words, it measures how close predictions are to the eventual outcomes \cite{willmott2005advantages}. MAE can simply be computed by:
$$MAE = {\frac{1} {N}{\sum\limits_{i = 1}^N {|x_{pred} - x_{obs} } }| } $$
where:\\
\textit{N}: number of instances\\
\textit{ $x_{pred}$}: prediction for instance x\\
\textit{$x_{obs}$}: observed value (ground truth) for instance x
\end{enumerate}

\noindent Having chosen error measurements explained, for the task of counting even digits, using 800,000 training samples and 200,000 test images, and after 1,600,000 iterations, our model obtained the results shown in table~\ref{tab:res}.


\begin{table}[H]
\centering
\small\sffamily
\begin{tabular}{llr}
%\toprule
\multicolumn{2}{c}{\textbf{\textbf{Results}}} \\
\bottomrule
\textbf{Error measurement}        & \textbf{Error} \\
\bottomrule
%\midrule
Euclidean loss           & 0.20  \\
Mean squared error       & 0.20  \\
Mean absolute error      & 0.20  \\
\bottomrule
\end{tabular}
\caption{}
\label{tab:res}
\end{table} 

Accordingly, the spread-error plot corresponding to the model is provided.

\begin{figure}[H]
	\centering
	{\includegraphics[width=0.7\textwidth]{images/1}}
	\caption{}
	\label{fig:splot}
\end{figure}

As you may observe in Figure~\ref{fig:splot}, most
of the frames are correctly labeled. Moreover, most of the errors correspond to adjacent number values.

\noindent A similar experiment was done by \citeauthor*{segui2015learning}. In their work, they used images with up to 5 digits with slightly different architecture. Also each digit in the images had a dimension of $28\times28$. They approached the problem as a classification task. 

\indent Therefore, in order to provide a fair comparison, we demonstrate how far each model's accuracy stands from random. These comparison has been shown in the table~\ref{tab:comp}.
%the difference the accuracy of both experiments from random, and from there we draw a proportional comparison.  These comparison has been shown in the table~\ref{tab:comp}.

\begin{table}[H]
\centering
\small\sffamily
\begin{tabular}{llr}
%\toprule
\multicolumn{3}{c}{\textbf{\textbf{Performance Comparison}}} \\
\bottomrule
\textbf{Experiments}  & \textbf{Chance} & \textbf{Accuracy} \\
\bottomrule
%\midrule
Our results           & 0.20 & 0.30 \\
Results obtained by \citeauthor*{segui2015learning}       & 0.20 & 0.30 \\

\bottomrule
\end{tabular}
\caption{.}
\label{tab:comp}
\end{table} 

\textbf{EXPLANATION OF THE COMPARISON}

\subsection{Conclusion}

\textbf{CONCLUSION AFTER THE EXPERIMENT} 

\section{Counting pedestrians in a walkway}

The first experiment proved that features can be learned automatically using deep CNN. It also shows that these deep features can be used to detect the object of interest in different but similar tasks. However, the use of deep architectures for fully supervised learning problems requires a large amount of annotated data. At the moment, for crowd counting problems, such data does not exist for research purposes. 

Hence, we propose a crowd counting problem using synthetic dataset in order to examine first, how well the model trained with synthetic images would perform. And then, whether the model is applicable for a real-world crowd counting scenario. We believe that this experiment will enlighten and address these questions. 

\subsection{Datasets} 

As fully discussed in chapter~\ref{subsec:synped}, for the particular case study, we synthetically created a dataset of 1 million images. To recap, the dataset specifications are the following:
%\todo{no need to repeat the information, but i like the description of the dataset here better}, DONT UNDERSTAND WHAT YOU MEAN??!!!!
\begin{itemize}
\item Each image is in gray-scale, $158\times158$ pixels, normalized, and contains up to 29 pedestrians in a walkway. Images are labeled with the number of pedestrians present in the image.  
\item Pedestrians' location in the images are center-based. A mask (region of interest) has been applied to images that defines the area in which the pedestrians are counted. Thus, the person is labeled if it's center is put in the region of interest.   
\item Out of 1 million images, 800,000 are considered for training and 200,000 for testing sets.
\end{itemize} 

\noindent In addition, in order to examine the performance of our model in a real-world scenario, we used UCSD crowd counting dataset \cite{chan2008privacy} with the underneath properties:

\begin{itemize}
\item The dataset consists of 3375 video frames shot by a stationary camera. Each image has up to 29 pedestrians present in the image.
\item Images are in gray-scale, resized to $158\times158$ pixels, normalized and filtered by the same mask (region of interest) as the other dataset. Labels are the number pedestrians placed in the region of interest.
\item Pedestrians annotation is center-based. The ones inside the mask are labeled.  
\item Images are continuous video frames of 20 different videos. Each video contains up to 200 images.  
\end{itemize}

Once again, to improve the training time, we converted the synthetic data into LMDB format for Caffe to read it in the fastest way.


\subsection{The learning process}

As fully described in chapter~\ref{subsec:ucsdarch}, we implemented a deep convolutional neural network in Caffe to learn the number of pedestrians in the walkway. As a reminder, the designed network has the components mentioned in table~\ref{myucsd3}. The network trains with stochastic gradient descent and our justification for the setting the solver parameters (hyper-parameters) remains the same as previous experiment (explained in section~\ref{subsec:ucsdarch}). However, once again table~\ref{hypar4} provides a review of the settings for network's hyper-parameters.

\begin{table}[!htb]
    \begin{minipage}{.5\linewidth}
		\begin{table}[H]
			\centering
			\begin{tabular}{ |p{2cm}|p{2cm}| }
			\hline 
			\multicolumn{2}{|c|}{\textbf{Network parameters}} \\
			\hline
			\hline
			\textbf{Layers} & \textbf{setting }\\
			\hline
			Conv1 & $20\times15\times15$\\
			\hline
			ReLU1 & max(x,0)  \\
			\hline
			LRN1 & $\alpha$=0.0001, $\beta$=0.75\\
			\hline
			Pool1    & $max(2\times2)$ \\
			\hline
			Conv2 & $50\times3\times3$\\
			\hline
			ReLU2 & max(x,0)  \\
			\hline
			LRN2 & $\alpha$=0.0001, $\beta$=0.75\\
			\hline
			Pool2    & $max(2\times2)$ \\
			\hline
			IP1 & 64 outputs \\
			\hline
			ReLU3 & max(x,0)  \\
			\hline
			IP2 & 1 outputs \\
			\hline
			\end{tabular}
				\caption{Proposed architecture's settings.}
				\label{hypar4}
		\end{table}
    \end{minipage}%
    \begin{minipage}{.5\linewidth}
		\begin{table}[H]
			\centering
			\begin{tabular}{ |p{3.8cm}|p{1.7cm}| }
			\hline 
			\multicolumn{2}{|c|}{\textbf{Network hyper-parameters}} \\
			\hline
			\hline
			\textbf{Hyper-parameters} & \textbf{setting }\\
			\hline
			Learning rate & 0.0001\\
			\hline
			Learning policy    & \textit{Multi\_step} \\
			\hline
			step size   & 40000 \\
			\hline
			Momentum & $\mu = 0.9$\\
			\hline
			Weight decay & 0.0005 \\
			\hline
			Batch size & 256 \\
			\hline
			Iterations & 1600000 \\
			\hline
			\end{tabular}
				\caption{Proposed settings for network's hyper-parameters.}
				\label{myucsd3}
		\end{table}
    \end{minipage} 
        %\caption{Global caption}
\end{table}


%The model's optimization method and parameters (solver parameters) are set similar to the learning to count problem. The network trains with stochastic gradient descent and our justification for the setting the solver parameters remains the same previous experiment (explained in section~\ref{solv:param}). Therefore, the repetitive explanations are skipped and only the final settings are described in table~\ref{tab:solvparam}.

%\begin{table}[H]
%	\centering
%			\caption{The solver parameters settings for even digit counting problem.}
%	\begin{tabular}{ |p{2cm}|p{2cm}| }
%	\hline 
%	\multicolumn{2}{|c|}{\textbf{Optimization parameters}} \\
%	\hline
%	\hline
%	\textbf{Parameter} & \textbf{value}\\
%	\hline
%	base\_lr & 0.0001\\
%	\hline
%	batch size & 256\\
%	\hline
%	momentum & 0.9  \\
%	\hline
%	weight decay & 0.0005\\
%	\hline
%	step size   & 40000 \\
%	\hline
%	gamma($\gamma$) & 0.1\\
%	\hline
%	iterations & 1,600,000\\
%	\hline
%	\end{tabular}
%
%		\label{tab:solvparam}
%\end{table}

\noindent Similarly, we used GPU NVIDIA TESLA K40 to train our network. After 5 days of training, we obtained the following learning curves :

\begin{figure}[H]
	\centering
	{\includegraphics[width=0.7\textwidth]{images/1}}
	\caption{}
	\label{fig:ucsdcurve}
\end{figure}

\noindent The network was able to learn sufficient features corresponding to the pedestrians. To illustrate, the learned features for one input sample, and after each convolutional layer in the network, has been depicted in the following figures: 



\begin{figure}[H]
	\centering
	{\includegraphics[width=0.7\textwidth]{images/1}}
	\caption{}
	\label{fig:feats}
\end{figure}

And finally, the network along with the outputs of different layers is shown in a schematic form in figure~\ref{fig:ucsdout}.

\begin{figure}[H]
	\centering
	{\includegraphics[width=0.7\textwidth]{images/1}}
	\caption{}
	\label{fig:ucsdout}
\end{figure}

\textbf{A SHORT TRANSITIVE PARAGRAPH TO GO TO RESULTS}

\subsection{Experimental results}

Equivalently to the first analysis, we use mean squared error, mean absolute error and the default Euclidean loss function to present the results obtained in this experiment. Having that said, the following results were obtained:

\begin{table}[H]
\centering
\small\sffamily
\begin{tabular}{llr}
%\toprule
\multicolumn{2}{c}{\textbf{\textbf{Results}}} \\
\bottomrule
\textbf{Error measurement}        & \textbf{Error} \\
\bottomrule
%\midrule
Euclidean loss           & 0.20  \\
Mean squared error       & 0.20  \\
Mean absolute error      & 0.20  \\
\bottomrule
\end{tabular}
\caption{Table caption font is different from the normal text font in order to get a better differentiation -- I like it that way.}
\label{tab:res}
\end{table} 

These results were attained after 1,600,000 iterations on 800,000 train and 200,000 test sets. Correspondingly, figure~\ref{fig:splot} depicts the spread-error plot of the model performance on the test set.

\begin{figure}[H]
	\centering
	{\includegraphics[width=0.7\textwidth]{images/1}}
	\caption{}
	\label{fig:splot}
\end{figure}

As you may notice, the network's predictions closely follow the target values. Also a direct correlation between the number of pedestrians and error deviation can be inferred from the spread-error plot which is reasonable due to the more overlapping in the crowded scenes. 

\noindent In state-of-the-art, \citeauthor*{segui2015learning} did a similar experiment with maximum 25 pedestrians in each image. Beside that, the difference between their work with this analysis, mainly revolves around the architecture design. Nonetheless, the table of comparison between these two approaches are shown in below:
 
\begin{table}[H]
\centering
\small\sffamily
\begin{tabular}{llr}
%\toprule
\multicolumn{3}{c}{\textbf{\textbf{Performance Comparison}}} \\
\bottomrule
\textbf{Experiments}  & \textbf{Chance} & \textbf{Accuracy} \\
\bottomrule
%\midrule
Euclidean loss           & 0.20 & 0.30 \\
Mean squared error       & 0.20 & 0.30 \\
\bottomrule
\end{tabular}
\caption{Table caption font is different from the normal text font in order to get a better differentiation -- I like it that way.}
\label{tab:}
\end{table} 

As you may observe, although we there are more pedestrians in our images, our model shows a more promising performance. 

\noindent As an innovative part of this Master thesis, we were interested to figure out if we have been able to overcome the exhaustive labeling in problems such as crowd counting. For this reason, we tested our model on a real UCSD crowd counting dataset \cite{chan2008privacy}. Table~\ref{tab:ucsdreal} demonstrates the performance of our model on UCSD dataset.


\begin{table}[H]
\centering
\small\sffamily
\begin{tabular}{llr}
%\toprule
\multicolumn{2}{c}{\textbf{\textbf{Results}}} \\
\bottomrule
\textbf{Error measurement}        & \textbf{Error} \\
\bottomrule
%\midrule
Euclidean loss           & 0.20  \\
Mean squared error       & 0.20  \\
Mean absolute error      & 0.20  \\
\bottomrule
\end{tabular}
\caption{Table caption font is different from the normal text font in order to get a better differentiation -- I like it that way.}
\label{tab:ucsdreal}
\end{table} 

As it can be inferred from the above table, the results are convincing and rational given the results we achieved on the synthetic test set and 
the inevitable differences between the synthetic and real datasets.


\noindent Moreover, we compared our model with \citeauthor*{chan2008privacy} research study in \cite{chan2008privacy} regarding crowd counting using exhaustive labeling and hand-crafted feature detectors using UCSD dataset. One difference we need to mention is that in their work, they computed the results for pedestrians towards camera and away from camera separately. However, in our dataset, we consider all pedestrians as one. Hence, we compare the average results of people towards and away from camera with our results. This comparison has been drawn in the following table:

\begin{table}[H]
\centering
\small\sffamily
\begin{tabular}{llr}
%\toprule
\multicolumn{3}{c}{\textbf{\textbf{Performance Comparison}}} \\
\bottomrule
\textbf{Experiments}  & \textbf{Chance} & \textbf{Accuracy} \\
\bottomrule
%\midrule
Euclidean loss           & 0.20 & 0.30 \\
Mean squared error       & 0.20 & 0.30 \\

\bottomrule
\end{tabular}
\caption{Table caption font is different from the normal text font in order to get a better differentiation -- I like it that way.}
\label{tab:realcomp}
\end{table} 

\subsection{Conclusion}

\textbf{CONCLUSION AFTER THE EXPERIMENT}

